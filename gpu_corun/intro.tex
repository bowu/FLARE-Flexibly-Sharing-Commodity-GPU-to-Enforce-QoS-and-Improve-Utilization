
\section{Introduction}
\vspace{-.2cm}
Datacenters are gaining increasing popularity as they significantly reduce the computation and storage 
cost for clients. However, the tremendous up-front investment in servers accounts 
for 50-70\% of the total cost of ownership~\cite{datacenter_cost}. 
The problem is exacerbated by the wide adoption of expensive high-end GPUs to 
leverage the massive parallelism to accelerate various types of workloads, 
such as deep neural networks %~\cite{mxnet} 
and graph analytics~\cite{Wang:PPoPP2015,Han:PACT2017}. 
Unfortunately, while CPU utilization in servers is already low (ranging from 10\% to 70\%~\cite{Lo:ISCA2014}), GPU under-utilization is more severe due to the complex dynamic behaviors of GPU applications~\cite{Chen+:ASPLOS16}.

A fundamental cause of hardware under-utilization is the strict QoS requirements of latency-critical (LC) applications (e.g., web services and deep learning inference). %~\cite{deepcpu,grnn}). 
To meet the QoS target, a conservative scheduler will reserve the entire server for the LC application. A promising solution is multitasking, which co-locates best-effort (BE) applications together with the LC application to share the same server and hence the GPUs. However, the BE application may interfere with the LC application, resulting in unacceptable performance degradation for LC requests. Notably, when both co-running applications heavily use the GPU, the slowdown of the LC requests could be over 10x~\cite{Chen:ASPLOS2017,Wu:ASPLOS2017}.

%Many efforts have been devoted to predict whether co-locating a pair of BE and LC applications violates QoS on CPU servers~\cite{Mars:MICRO2011,Zhang:2014,Yang:2013,Subramanian:2015}. A common theme is to characterize potential resource contention between applications, based on which a runtime builds a model for performance degradation prediction. The global scheduler co-locates applications if the prediction shows that doing so would not violate the QoS requirement. Another line of works proposes controllers to dynamically partition resources between co-running applications to meet the QoS goal~\cite{Zhu+:ASPLOS16,Lo:2015,Delimitrou:2014}. Although existing multitasking research on CPU considers contention on various shared resources, including memory bandwidth, CPU cores, and network bandwidth, the findings do not directly apply to GPUs because of the unique architectural features (details in Section~\ref{sec:back}).

As far as we know, Baymax~\cite{Chen+:ASPLOS16} and Laius~\cite{laius} are the only software systems that enforce QoS for shared GPU systems. %They leverage performance models for both kernel execution and data transfers to coordinate GPU requests from co-running applications. 
Baymax assumes that the GPU is a non-preemptable processor and hence a long-running kernel reserves the entire GPU. However, a high-end GPU has tens of streaming multi-processors (SMs), which cannot be fully utilized by a single kernel. As we show in Section~\ref{sec:back}, GPU kernels may scale poorly in terms of SMs or threads within an SM. Laius takes advantage of the hardware-based partitioning capability but is limited to SM-level partitioning, therefore failing to addressing the scalability issues within SMs.

In this paper, we aim at improving GPU utilization by flexibly partitioning the abundant computational resource between co-running BE and LC applications. We assume that the source code of BE is available and an BE application is constantly running on the GPU when the LC application arrives. Instead of only coordinating GPU kernel executions, we allow a BE application to yield just enough resource to meet the QoS target of the LC kernel. To achieve this goal, we face multiple challenges. First, while one only needs to consider a 1-D resource space for CPU core allocation~\cite{Mars:MICRO2011}, the GPU has many SMs and each SM concurrently runs several groups of threads (i.e., thread blocks), thus forming a 2-D resource space. Second, since the GPU by default runs the launched kernels in an FIFO manner, a kernel from the BE application may use up all SMs, thus blocking the kernel of the LC application. We need to design a software mechanism to enable the two kernels to run simultaneously on different parts of the GPU. Third, the co-running kernels interfere with each other on a variety of hardware resources, including shared interconnect, L1 cache, L2 cache, streaming cores, and device memory. Therefore, quantifying the performance degradation given a partitioning configuration is difficult. Finally, we try to enforce QoS and maximize utilization which are two conflicting goals. Specifically, by allocating more resources to the LC application, we have a better chance to meet the QoS goal. But it probably reduces the overall throughput at the same time.

To overcome the challenges and improve utilization of {\em commodity} GPUs, we design and implement a software system, FLARE, which enables flexible GPU sharing, meets QoS goals for LC applications, and maximizes throughput for BE applications. FLARE transforms the kernel of the BE application to be able to yield $k$ ($1 \leq k \leq MaxBlksPerSM$) thread blocks on a subset of $n$ SMs ($1 \leq n \leq MaxSMs$). The pair $n\_k$ is called a configuration. The threads of the LC kernel can then be scheduled to run on the released hardware resource. The key novelty of FLARE is its intelligent runtime to quickly figure out the optimal GPU resource partitioning strategy by avoiding pitfalls from two popular existing approaches as follows. The performance model-based approach uses offline training to predict the best configuration~\cite{Chen+:ASPLOS16,Zhang:2014}, but its accuracy may suffer from input sensitivity and complicated hardware contention. On the other hand, a pure dynamic approach (e.g., online profiling and adjusting~\cite{Lo:2015,Zhu+:ASPLOS16}) may not be responsive enough. Worse, it may explore detrimental configurations that lead to hampered QoS or hardware under-utilization. FLARE employs a hybrid methodology. It uses microbenchmarks to characterize the co-run performance degradation space, so given two co-running kernels it quickly predicts an initial configuration to use. Then FLARE leverages the degradation space to dynamically search for the optimal configuration. We show in comprehensive experiments that FLARE outperforms the preemption-based approach while satisfying the QoS targets.
